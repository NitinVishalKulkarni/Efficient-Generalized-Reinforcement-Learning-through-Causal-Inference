# Efficient-Generalized-Reinforcement-Learning-through-Causal-Inference
One of the fundamental challenges in the successful deployment of reinforcement learning systems in the real world is their ability to generalize. Although the state-of-the-art deep reinforcement learning algorithms can solve complex tasks, they require a large number of interactions with the environment and don't perform well on new environments. These algorithms latch onto the specifics of the environments they are trained on instead of learning general concepts. We propose that generalization to out-of-distribution (OOD) tasks can be achieved through causal inference. We demonstrate that our solution achieves generalization to a number of OOD tasks while being trained on just a few variations of the environments.
