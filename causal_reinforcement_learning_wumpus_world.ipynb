{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "nG6CnuVUsnlM",
    "outputId": "c7c90d79-a2fe-4d16-c657-8cbe08310bc6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import cv2\n",
    "from dowhy import CausalModel\n",
    "import econml\n",
    "import glob\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from time import time\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHkFG2QFsnlQ"
   },
   "source": [
    "### Wumpus World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PpW5m66TsnlS"
   },
   "outputs": [],
   "source": [
    "# Defining the Wumpus World Environment.\n",
    "class WumpusWorldEnvironment(gym.Env):\n",
    "    \"\"\"This class implements the Wumpus World environment.\"\"\"\n",
    "\n",
    "    def __init__(self, environment_type):\n",
    "        \"\"\"This method initializes the environment.\n",
    "\n",
    "        :param str environment_type: - (It can take two values: 1. 'training' 2. 'testing' indicating the type of\n",
    "                                    environment.)\"\"\"\n",
    "\n",
    "        self.environment_type = environment_type\n",
    "\n",
    "        if self.environment_type == 'training':\n",
    "            self.environment_width = 6\n",
    "            self.environment_height = 6\n",
    "            # This defines the total number of grid blocks in the environment.\n",
    "            self.observation_space = spaces.Discrete(self.environment_width * self.environment_height)\n",
    "            # This defines that there are 4 discrete actions that the agent can perform.\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "            self.number_of_agents = 1  # This defines the number of agents in the environment.\n",
    "            self.agent_pos = np.asarray([0, 0])  # This defines the agent's default initial position in the environment.\n",
    "            # This defines the positions of breeze in the environment.\n",
    "            self.breeze_pos = np.asarray([[1, 0], [3, 0], [5, 0], [2, 1], [4, 1], [1, 2], [3, 2], [5, 2], [0, 3],\n",
    "                                          [2, 3], [1, 4], [3, 4], [5, 4], [0, 5], [2, 5], [4, 5]])\n",
    "            self.gold_pos = np.asarray([4, 5])  # This defines the position of gold in the environment.\n",
    "            self.gold_quantity = 1  # This defines the quantity of gold.\n",
    "            # This defines the positions of pit in the environment.\n",
    "            self.pit_pos = np.asarray([[2, 0], [5, 1], [2, 2], [0, 4], [2, 4], [3, 5], [5, 5]])\n",
    "            # This defines the positions of stench in the environment.\n",
    "            self.stench_pos = np.asarray([[3, 2], [2, 3], [4, 3], [3, 4]])\n",
    "            self.wumpus_pos = np.asarray([3, 3])  # This defines the position of the Wumpus in the environment.\n",
    "            self.timesteps = 0  # This defines the steps the agent has taken during an episode.\n",
    "            self.max_timesteps = 1000  # This defines the maximum steps the agent can take during an episode.\n",
    "\n",
    "            # Creating the mapping from the co-ordinates to the state.\n",
    "            self.coordinates_state_mapping = {}\n",
    "            for i in range(self.environment_height):\n",
    "                for j in range(self.environment_width):\n",
    "                    self.coordinates_state_mapping[f'{np.asarray([j, i])}'] = i * self.environment_width + j\n",
    "\n",
    "            # Storing the terminal and non-terminal states.\n",
    "            self.terminal_states = []\n",
    "            self.non_terminal_states = []\n",
    "            for position in self.coordinates_state_mapping:\n",
    "                if np.array_equal(f'{self.wumpus_pos}', position) or np.array_equal(f'{self.gold_pos}', position) or \\\n",
    "                        any(np.array_equal(f'{self.pit_pos[i]}', position) for i in range(len(self.pit_pos))) or \\\n",
    "                        any(np.array_equal(f'{self.breeze_pos[i]}', position) for i in range(len(self.breeze_pos))) or \\\n",
    "                        any(np.array_equal(f'{self.stench_pos[i]}', position) for i in range(len(self.stench_pos))):\n",
    "                    self.terminal_states.append(self.coordinates_state_mapping[position])\n",
    "                else:\n",
    "                    self.non_terminal_states.append(self.coordinates_state_mapping[position])\n",
    "\n",
    "        elif self.environment_type == 'testing':\n",
    "            self.environment_width = 6\n",
    "            self.environment_height = 6\n",
    "            # This defines the total number of grid blocks in the environment.\n",
    "            self.observation_space = spaces.Discrete(self.environment_width * self.environment_height)\n",
    "            # This defines that there are 4 discrete actions that the agent can perform.\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "            self.number_of_agents = 1  # This defines the number of agents in the environment.\n",
    "            self.agent_pos = np.asarray([0, 0])  # This defines the agent's default initial position in the environment.\n",
    "            # This defines the positions of breeze in the environment.\n",
    "            self.breeze_pos = np.asarray(\n",
    "                [[1, 0], [3, 0], [5, 0], [2, 1], [4, 1], [1, 2], [3, 2], [5, 2], [0, 3], [2, 3],\n",
    "                 [1, 4], [3, 4], [5, 4], [0, 5], [2, 5], [4, 5]])\n",
    "            self.gold_pos = np.asarray([0, 5])  # This defines the position of gold in the environment.\n",
    "            self.gold_quantity = 1  # This defines the quantity of gold.\n",
    "            # This defines the positions of pit in the environment.\n",
    "            self.pit_pos = np.asarray([[2, 0], [5, 1], [2, 2], [0, 4], [2, 4], [3, 5], [5, 5]])\n",
    "            # This defines the positions of stench in the environment.\n",
    "            self.stench_pos = np.asarray([[3, 2], [2, 3], [4, 3], [3, 4]])\n",
    "            self.wumpus_pos = np.asarray([3, 3])  # This defines the position of the Wumpus in the environment.\n",
    "            self.timesteps = 0  # This defines the steps the agent has taken during an episode.\n",
    "            self.max_timesteps = 1000  # This defines the maximum steps the agent can take during an episode.\n",
    "\n",
    "            # Creating the mapping from the co-ordinates to the state.\n",
    "            self.coordinates_state_mapping = {}\n",
    "            for i in range(self.environment_height):\n",
    "                for j in range(self.environment_width):\n",
    "                    self.coordinates_state_mapping[f'{np.asarray([j, i])}'] = i * self.environment_width + j\n",
    "\n",
    "            # Storing the terminal and non-terminal states.\n",
    "            self.terminal_states = []\n",
    "            self.non_terminal_states = []\n",
    "            for position in self.coordinates_state_mapping:\n",
    "                if np.array_equal(f'{self.wumpus_pos}', position) or np.array_equal(f'{self.gold_pos}', position) or \\\n",
    "                        any(np.array_equal(f'{self.pit_pos[i]}', position) for i in range(len(self.pit_pos))) or \\\n",
    "                        any(np.array_equal(f'{self.breeze_pos[i]}', position) for i in range(len(self.breeze_pos))) or \\\n",
    "                        any(np.array_equal(f'{self.stench_pos[i]}', position) for i in range(len(self.stench_pos))):\n",
    "                    self.terminal_states.append(self.coordinates_state_mapping[position])\n",
    "                else:\n",
    "                    self.non_terminal_states.append(self.coordinates_state_mapping[position])\n",
    "\n",
    "    def partially_observable_state(self, agent_position):\n",
    "        \"\"\"This method returns the array to append to the states for partially observable MDP.\n",
    "        :param arr agent_position: Integer representation of the state from the environment.\n",
    "\n",
    "        :return: arr observation: Array representing the partial observation.\"\"\"\n",
    "\n",
    "        observation = np.zeros(9 * 5)\n",
    "        positions_to_evaluate = [agent_position, [agent_position[0] - 1, agent_position[1]],\n",
    "                                 [agent_position[0] - 1, agent_position[1] + 1],\n",
    "                                 [agent_position[0], agent_position[1] + 1],\n",
    "                                 [agent_position[0] + 1, agent_position[1] + 1],\n",
    "                                 [agent_position[0] + 1, agent_position[1]],\n",
    "                                 [agent_position[0] + 1, agent_position[1] - 1],\n",
    "                                 [agent_position[0], agent_position[1] - 1],\n",
    "                                 [agent_position[0] - 1, agent_position[1] - 1]]\n",
    "\n",
    "        index = 0\n",
    "        for position in positions_to_evaluate:\n",
    "            if any(np.array_equal(position, self.breeze_pos[x]) for x in range(len(self.breeze_pos))):\n",
    "                observation[index] = 1\n",
    "            if any(np.array_equal(position, self.stench_pos[x]) for x in range(len(self.stench_pos))):\n",
    "                observation[index + 1] = 1\n",
    "            if any(np.array_equal(position, self.pit_pos[x]) for x in range(len(self.pit_pos))):\n",
    "                observation[index + 2] = 1\n",
    "            if np.array_equal(position, self.wumpus_pos):\n",
    "                observation[index + 3] = 1\n",
    "            if np.array_equal(position, self.gold_pos):\n",
    "                observation[index + 4] = 1\n",
    "            index += 5\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def reset(self, random_start=False):\n",
    "        \"\"\"This method resets the agent position and returns the state as the observation.\n",
    "\n",
    "        :param bool random_start: - Boolean indicating whether the agent will start in a random or fixed position.\n",
    "\n",
    "        :returns arr observation: -  Array representing the partial observation.\"\"\"\n",
    "\n",
    "        if not random_start:\n",
    "            self.agent_pos = np.asarray([0, 0])  # Upon resetting the environment the agent's position is set to [0, 0].\n",
    "        else:\n",
    "            # Randomly selecting the agent's position.\n",
    "            random_state = random.choice(self.non_terminal_states)\n",
    "            self.agent_pos = np.asarray([random_state % self.environment_width,\n",
    "                                         int(np.floor(random_state / self.environment_width))])\n",
    "\n",
    "        observation = self.partially_observable_state(self.agent_pos)\n",
    "        self.timesteps = 0  # Resetting the number of steps taken by the agent.\n",
    "        self.gold_quantity = 1  # Resetting the Gold quantity to be 1.\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"This method implements what happens when the agent takes a particular action. It changes the agent's\n",
    "        position (While not allowing it to go out of the environment space.), maps the environment co-ordinates to a\n",
    "        state, defines the rewards for the various states, and determines when the episode ends.\n",
    "\n",
    "        :param int action: - Integer in the range 0 to 3 inclusive representing the different actions the agent can\n",
    "        take.\n",
    "\n",
    "        :returns arr observation: - Array representing the partial observation.\n",
    "                 int reward: - Integer value that's used to measure the performance of the agent.\n",
    "                 bool done: - Boolean describing whether the episode has ended.\n",
    "                 dict info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        # Describing the outcomes of the various possible actions.\n",
    "        if action == 0:\n",
    "            self.agent_pos[0] += 1  # This action causes the agent to go right.\n",
    "        if action == 1:\n",
    "            self.agent_pos[0] -= 1  # This action causes the agent to go left.\n",
    "        if action == 2:\n",
    "            self.agent_pos[1] += 1  # This action causes the agent to go up.\n",
    "        if action == 3:\n",
    "            self.agent_pos[1] -= 1  # This action causes the agent to go down.\n",
    "\n",
    "        # Ensuring that the agent doesn't go out of the environment.\n",
    "        self.agent_pos = np.clip(self.agent_pos, a_min=[0, 0],\n",
    "                                 a_max=[self.environment_width - 1, self.environment_height - 1])\n",
    "        observation = self.partially_observable_state(self.agent_pos)\n",
    "\n",
    "        self.timesteps += 1  # Increasing the total number of steps taken by the agent.\n",
    "\n",
    "        reward = 0\n",
    "        # Setting the reward to 10 if the agent reaches the gold.\n",
    "        if np.array_equal(self.agent_pos, self.gold_pos) and self.gold_quantity > 0:\n",
    "            self.gold_quantity -= 1\n",
    "            reward = 1000\n",
    "\n",
    "        for i in range(len(self.pit_pos)):  # Setting the reward to -1 if the agent falls in the pit.\n",
    "            if np.array_equal(self.agent_pos, self.pit_pos[i]):\n",
    "                reward = -50\n",
    "\n",
    "        # Setting the reward to -1 if the agent is killed by Wumpus.\n",
    "        if np.array_equal(self.agent_pos, self.wumpus_pos):\n",
    "            reward = -100\n",
    "\n",
    "        # The episode terminates when the agent reaches the Gold, or is killed by the Wumpus, falls into the pit, or\n",
    "        # takes more than 10 steps.\n",
    "        if self.gold_quantity == 0 or \\\n",
    "                np.array_equal(self.agent_pos, self.wumpus_pos):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        for i in range(len(self.pit_pos)):\n",
    "            if np.array_equal(self.agent_pos, self.pit_pos[i]):\n",
    "                done = True\n",
    "        if self.timesteps == self.max_timesteps:\n",
    "            done = True\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def render(self, mode='human', plot=False):\n",
    "        \"\"\"This method renders the environment.\n",
    "\n",
    "        :param str mode: 'human' renders to the current display or terminal and returns nothing.\n",
    "        :param bool plot: Boolean indicating whether we show a plot or not. If False, the method returns a resized NumPy\n",
    "                     array representation of the environment to be used as the state. If True it plots the environment.\n",
    "\n",
    "        :returns arr preprocessed_image: Grayscale NumPy array representation of the environment.\"\"\"\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 15))  # Initializing the figure.\n",
    "        ax.set_xlim(0, 6)  # Setting the limit on the x-axis.\n",
    "        ax.set_ylim(0, 6)  # Setting the limit on the y-axis.\n",
    "\n",
    "        def plot_image(plot_pos):\n",
    "            \"\"\"This is a helper function to render the environment. It checks which objects are in a particular\n",
    "            position on the grid and renders the appropriate image.\n",
    "\n",
    "            :param arr plot_pos: Co-ordinates of the grid position which needs to be rendered.\"\"\"\n",
    "\n",
    "            # Initially setting every object to not be plotted.\n",
    "            plot_agent, plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus = \\\n",
    "                False, False, False, False, False, False\n",
    "\n",
    "            # Checking which objects need to be plotted by comparing their positions.\n",
    "            if np.array_equal(self.agent_pos, plot_pos):\n",
    "                plot_agent = True\n",
    "            if any(np.array_equal(self.breeze_pos[i], plot_pos) for i in range(len(self.breeze_pos))):\n",
    "                plot_breeze = True\n",
    "            if self.gold_quantity > 0:  # Gold isn't plotted if it has already been picked by one of the agents.\n",
    "                if np.array_equal(plot_pos, self.gold_pos):\n",
    "                    plot_gold = True\n",
    "            if any(np.array_equal(self.pit_pos[i], plot_pos) for i in range(len(self.pit_pos))):\n",
    "                plot_pit = True\n",
    "            if any(np.array_equal(self.stench_pos[i], plot_pos) for i in range(len(self.stench_pos))):\n",
    "                plot_stench = True\n",
    "            if np.array_equal(plot_pos, self.wumpus_pos):\n",
    "                plot_wumpus = True\n",
    "\n",
    "            # Plot for Agent.\n",
    "            if plot_agent and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent.png'), zoom=0.28),\n",
    "                                       np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(agent)\n",
    "\n",
    "            # Plot for Breeze.\n",
    "            elif plot_breeze and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n",
    "                breeze = AnnotationBbox(OffsetImage(plt.imread('./images/breeze.png'), zoom=0.28),\n",
    "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(breeze)\n",
    "\n",
    "            # Plot for Gold.\n",
    "            elif plot_gold and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_breeze, plot_pit, plot_stench, plot_wumpus]):\n",
    "                gold = AnnotationBbox(OffsetImage(plt.imread('./images/gold.png'), zoom=0.28),\n",
    "                                      np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(gold)\n",
    "\n",
    "            # Plot for Pit.\n",
    "            elif plot_pit and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_breeze, plot_gold, plot_stench, plot_wumpus]):\n",
    "                pit = AnnotationBbox(OffsetImage(plt.imread('./images/pit.png'), zoom=0.28),\n",
    "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(pit)\n",
    "\n",
    "            # Plot for Stench.\n",
    "            elif plot_stench and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_breeze, plot_gold, plot_pit, plot_wumpus]):\n",
    "                stench = AnnotationBbox(OffsetImage(plt.imread('./images/stench.png'), zoom=0.28),\n",
    "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(stench)\n",
    "\n",
    "            # Plot for Wumpus.\n",
    "            elif plot_wumpus and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_breeze, plot_gold, plot_pit, plot_stench]):\n",
    "                wumpus = AnnotationBbox(OffsetImage(plt.imread('./images/wumpus.png'), zoom=0.28),\n",
    "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(wumpus)\n",
    "\n",
    "            # Plot for Agent and Breeze.\n",
    "            elif all(item for item in [plot_agent, plot_breeze]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_gold, plot_pit, plot_stench, plot_wumpus]):\n",
    "                agent_breeze = AnnotationBbox(OffsetImage(plt.imread('./images/agent_breeze.png'), zoom=0.28),\n",
    "                                              np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(agent_breeze)\n",
    "\n",
    "            # Plot for Agent and Pit.\n",
    "            elif all(item for item in [plot_agent, plot_pit]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_breeze, plot_gold, plot_stench, plot_wumpus]):\n",
    "                agent_pit = AnnotationBbox(OffsetImage(plt.imread('./images/agent_dead_pit.png'), zoom=0.28),\n",
    "                                           np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(agent_pit)\n",
    "\n",
    "            # Plot for Agent and Stench.\n",
    "            elif all(item for item in [plot_agent, plot_stench]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_breeze, plot_gold, plot_pit, plot_wumpus]):\n",
    "                agent_stench = AnnotationBbox(OffsetImage(plt.imread('./images/agent_stench.png'), zoom=0.28),\n",
    "                                              np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(agent_stench)\n",
    "\n",
    "            # Plot for Agent, Breeze and Stench.\n",
    "            elif all(item for item in [plot_agent, plot_breeze, plot_stench]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_gold, plot_pit, plot_wumpus]):\n",
    "                agent_breeze_stench = AnnotationBbox(OffsetImage(plt.imread('./images/agent_breeze_stench.png'),\n",
    "                                                                 zoom=0.28), np.add(plot_pos, [0.5, 0.5]),\n",
    "                                                     frameon=False)\n",
    "                ax.add_artist(agent_breeze_stench)\n",
    "\n",
    "            # Plot for Agent and Wumpus.\n",
    "            elif all(item for item in [plot_agent, plot_wumpus]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_gold, plot_pit, plot_stench, plot_breeze]):\n",
    "                agent_wumpus = AnnotationBbox(OffsetImage(plt.imread('./images/agent_dead_wumpus_alive.png'),\n",
    "                                                          zoom=0.28), np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(agent_wumpus)\n",
    "\n",
    "            # Plot for Breeze and Gold.\n",
    "            elif all(item for item in [plot_breeze, plot_gold]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_pit, plot_stench, plot_wumpus]):\n",
    "                breeze_gold = AnnotationBbox(OffsetImage(plt.imread('./images/breeze_gold.png'), zoom=0.28),\n",
    "                                             np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(breeze_gold)\n",
    "\n",
    "            # Plot for Breeze and Stench.\n",
    "            elif all(item for item in [plot_breeze, plot_stench]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_gold, plot_pit, plot_wumpus]):\n",
    "                breeze_stench = AnnotationBbox(OffsetImage(plt.imread('./images/breeze_stench.png'), zoom=0.28),\n",
    "                                               np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(breeze_stench)\n",
    "\n",
    "            # Plot for Breeze, Stench, and Gold.\n",
    "            elif all(item for item in [plot_breeze, plot_gold, plot_stench]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_pit, plot_wumpus]):\n",
    "                breeze_gold_stench = AnnotationBbox(OffsetImage(plt.imread('./images/breeze_gold_stench.png'),\n",
    "                                                                zoom=0.28), np.add(plot_pos, [0.5, 0.5]),\n",
    "                                                    frameon=False)\n",
    "                ax.add_artist(breeze_gold_stench)\n",
    "\n",
    "            # Plot for Stench and Gold.\n",
    "            elif all(item for item in [plot_stench, plot_gold]) and \\\n",
    "                    all(not item for item in\n",
    "                        [plot_agent, plot_breeze, plot_pit, plot_wumpus]):\n",
    "                stench_gold = AnnotationBbox(OffsetImage(plt.imread('./images/stench_gold.png'), zoom=0.28),\n",
    "                                             np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
    "                ax.add_artist(stench_gold)\n",
    "\n",
    "        coordinates_state_mapping_2 = {}\n",
    "        for j in range(self.environment_height * self.environment_width):\n",
    "            coordinates_state_mapping_2[j] = np.asarray(\n",
    "                [j % self.environment_width, int(np.floor(j / self.environment_width))])\n",
    "\n",
    "        # Rendering the images for all states.\n",
    "        for position in coordinates_state_mapping_2:\n",
    "            plot_image(coordinates_state_mapping_2[position])\n",
    "\n",
    "        plt.xticks([0, 1, 2, 3, 4, 5])  # Specifying the ticks on the x-axis.\n",
    "        plt.yticks([0, 1, 2, 3, 4, 5])  # Specifying the ticks on the y-axis.\n",
    "        plt.grid()  # Setting the plot to be of the type 'grid'.\n",
    "\n",
    "        if plot:  # Displaying the plot.\n",
    "            plt.show()\n",
    "        else:  # Returning the preprocessed image representation of the environment.\n",
    "            fig.canvas.draw()\n",
    "            img = np.array(fig.canvas.renderer.buffer_rgba())[:, :, :1]\n",
    "            width = int(img.shape[1] * 84 / 1000)\n",
    "            height = int(img.shape[0] * 84 / 1000)\n",
    "            dim = (width, height)\n",
    "            # noinspection PyUnresolvedReferences\n",
    "            preprocessed_image = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "            return preprocessed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4GILwgR8snlY"
   },
   "outputs": [],
   "source": [
    "class GenerateCausalMap:\n",
    "    \"\"\"This class performs the causal inference and generates the casual map.\"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"This method initializes the environment variables.\"\"\"\n",
    "\n",
    "        self.environment = environment\n",
    "        self.samples_to_collect = 25000\n",
    "\n",
    "        self.history = {'Action Right': [], 'Action Left': [], 'Action Up': [], 'Action Down': [],\n",
    "                        'State At Breeze': [], 'State At Stench': [], 'State At Pit': [], 'State At Wumpus': [],\n",
    "                        'State At Gold': [],\n",
    "                        'State Left Breeze': [], 'State Left Stench': [], 'State Left Pit': [], 'State Left Wumpus': [],\n",
    "                        'State Left Gold': [],\n",
    "                        'State Top Left Breeze': [], 'State Top Left Stench': [], 'State Top Left Pit': [],\n",
    "                        'State Top Left Wumpus': [], 'State Top Left Gold': [],\n",
    "                        'State Up Breeze': [], 'State Up Stench': [], 'State Up Pit': [], 'State Up Wumpus': [],\n",
    "                        'State Up Gold': [],\n",
    "                        'State Top Right Breeze': [], 'State Top Right Stench': [], 'State Top Right Pit': [],\n",
    "                        'State Top Right Wumpus': [], 'State Top Right Gold': [],\n",
    "                        'State Right Breeze': [], 'State Right Stench': [], 'State Right Pit': [],\n",
    "                        'State Right Wumpus': [], 'State Right Gold': [],\n",
    "                        'State Bottom Right Breeze': [], 'State Bottom Right Stench': [], 'State Bottom Right Pit': [],\n",
    "                        'State Bottom Right Wumpus': [], 'State Bottom Right Gold': [],\n",
    "                        'State Down Breeze': [], 'State Down Stench': [], 'State Down Pit': [], 'State Down Wumpus': [],\n",
    "                        'State Down Gold': [],\n",
    "                        'State Bottom Left Breeze': [], 'State Bottom Left Stench': [], 'State Bottom Left Pit': [],\n",
    "                        'State Bottom Left Wumpus': [], 'State Bottom Left Gold': [],\n",
    "                        'Next State At Breeze': [], 'Next State At Stench': [], 'Next State At Pit': [],\n",
    "                        'Next State At Wumpus': [], 'Next State At Gold': [],\n",
    "                        'Next State Left Breeze': [], 'Next State Left Stench': [], 'Next State Left Pit': [],\n",
    "                        'Next State Left Wumpus': [], 'Next State Left Gold': [],\n",
    "                        'Next State Top Left Breeze': [], 'Next State Top Left Stench': [],\n",
    "                        'Next State Top Left Pit': [], 'Next State Top Left Wumpus': [], 'Next State Top Left Gold': [],\n",
    "                        'Next State Up Breeze': [], 'Next State Up Stench': [], 'Next State Up Pit': [],\n",
    "                        'Next State Up Wumpus': [], 'Next State Up Gold': [],\n",
    "                        'Next State Top Right Breeze': [], 'Next State Top Right Stench': [],\n",
    "                        'Next State Top Right Pit': [], 'Next State Top Right Wumpus': [],\n",
    "                        'Next State Top Right Gold': [],\n",
    "                        'Next State Right Breeze': [], 'Next State Right Stench': [], 'Next State Right Pit': [],\n",
    "                        'Next State Right Wumpus': [], 'Next State Right Gold': [],\n",
    "                        'Next State Bottom Right Breeze': [], 'Next State Bottom Right Stench': [],\n",
    "                        'Next State Bottom Right Pit': [], 'Next State Bottom Right Wumpus': [],\n",
    "                        'Next State Bottom Right Gold': [],\n",
    "                        'Next State Down Breeze': [], 'Next State Down Stench': [], 'Next State Down Pit': [],\n",
    "                        'Next State Down Wumpus': [], 'Next State Down Gold': [],\n",
    "                        'Next State Bottom Left Breeze': [], 'Next State Bottom Left Stench': [],\n",
    "                        'Next State Bottom Left Pit': [], 'Next State Bottom Left Wumpus': [],\n",
    "                        'Next State Bottom Left Gold': [],\n",
    "                        'Reward': [], 'Done': []}\n",
    "        self.data = pd.DataFrame.from_dict(self.history)\n",
    "        self.list_of_positions = ['At', 'Left', 'Top Left', 'Up', 'Top Right', 'Right', 'Bottom Right', 'Down',\n",
    "                                  'Bottom Left']\n",
    "\n",
    "    def generate_random_data(self):\n",
    "        \"\"\"This method generates data through random exploration.\"\"\"\n",
    "\n",
    "        while len(self.history['Action Right']) < self.samples_to_collect:\n",
    "            state = self.environment.reset(random_start=True)\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                action = self.environment.action_space.sample()\n",
    "\n",
    "                next_state, reward, done, info = self.environment.step(action)\n",
    "\n",
    "                if action == 0:\n",
    "                    self.history['Action Right'].append(True)\n",
    "                else:\n",
    "                    self.history['Action Right'].append(False)\n",
    "\n",
    "                if action == 1:\n",
    "                    self.history['Action Left'].append(True)\n",
    "                else:\n",
    "                    self.history['Action Left'].append(False)\n",
    "\n",
    "                if action == 2:\n",
    "                    self.history['Action Up'].append(True)\n",
    "                else:\n",
    "                    self.history['Action Up'].append(False)\n",
    "\n",
    "                if action == 3:\n",
    "                    self.history['Action Down'].append(True)\n",
    "                else:\n",
    "                    self.history['Action Down'].append(False)\n",
    "\n",
    "                self.history['Reward'].append(reward)\n",
    "                self.history['Done'].append(done)\n",
    "\n",
    "                index = 0\n",
    "                for i in range(9):\n",
    "\n",
    "                    if state[index] == 1:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Breeze'].append(True)\n",
    "                    elif state[index] == 0:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Breeze'].append(False)\n",
    "\n",
    "                    if state[index + 1] == 1:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Stench'].append(True)\n",
    "                    elif state[index + 1] == 0:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Stench'].append(False)\n",
    "\n",
    "                    if state[index + 2] == 1:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Pit'].append(True)\n",
    "                    elif state[index + 2] == 0:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Pit'].append(False)\n",
    "\n",
    "                    if state[index + 3] == 1:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Wumpus'].append(True)\n",
    "                    elif state[index + 3] == 0:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Wumpus'].append(False)\n",
    "\n",
    "                    if state[index + 4] == 1:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Gold'].append(True)\n",
    "                    elif state[index + 4] == 0:\n",
    "                        self.history['State ' + self.list_of_positions[i] + ' Gold'].append(False)\n",
    "\n",
    "                    if next_state[index] == 1:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Breeze'].append(True)\n",
    "                    elif next_state[index] == 0:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Breeze'].append(False)\n",
    "\n",
    "                    if next_state[index + 1] == 1:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Stench'].append(True)\n",
    "                    elif next_state[index + 1] == 0:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Stench'].append(False)\n",
    "\n",
    "                    if next_state[index + 2] == 1:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Pit'].append(True)\n",
    "                    elif next_state[index + 2] == 0:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Pit'].append(False)\n",
    "\n",
    "                    if next_state[index + 3] == 1:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Wumpus'].append(True)\n",
    "                    elif next_state[index + 3] == 0:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Wumpus'].append(False)\n",
    "\n",
    "                    if next_state[index + 4] == 1:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Gold'].append(True)\n",
    "                    elif next_state[index + 4] == 0:\n",
    "                        self.history['Next State ' + self.list_of_positions[i] + ' Gold'].append(False)\n",
    "                    index += 5\n",
    "                    \n",
    "                state = next_state\n",
    "\n",
    "                if len(self.history['Action Right']) == self.samples_to_collect:\n",
    "                    break\n",
    "                    \n",
    "        self.data = pd.DataFrame.from_dict(self.history)\n",
    "\n",
    "    def reward_to_object(self):\n",
    "        \"\"\"This method computes the causal estimate from the reward to the objects.\"\"\"\n",
    "\n",
    "        # Necessary dictionaries.\n",
    "        models = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimands = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimate_values = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        refutation_estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        environment_objects = ['Breeze', 'Gold', 'Pit', 'Stench', 'Wumpus']\n",
    "\n",
    "        # I. Create a causal model from the data and given graph.\n",
    "        for environment_object in environment_objects:\n",
    "            graph = f'graph[directed 1node[ id \"Reward\" label \"Reward\"]node[ id \"Next State At {environment_object}\"' \\\n",
    "                f' label \"Next State At {environment_object}\"]edge[source \"Reward\" target \"Next State At ' \\\n",
    "                f'{environment_object}\"]]'\n",
    "            model = CausalModel(data=self.data, treatment=['Reward'], outcome=[f'Next State At {environment_object}'],\n",
    "                                graph=graph)\n",
    "            models[f'{environment_object}'].append(model)\n",
    "\n",
    "        # II. Identify causal effect and return target estimands.\n",
    "        for environment_object in environment_objects:\n",
    "            estimands[f'{environment_object}'].append(models[f'{environment_object}'][0].identify_effect())\n",
    "\n",
    "        # III. Estimate the target estimand using a statistical method.\n",
    "        for environment_object in environment_objects:\n",
    "            estimate = models[f'{environment_object}'][0].estimate_effect(estimands[f'{environment_object}'][0],\n",
    "                                                                          method_name=\"backdoor.linear_regression\",\n",
    "                                                                          test_significance=True, control_value=0,\n",
    "                                                                          treatment_value=1)\n",
    "            estimates[f'{environment_object}'].append(estimate)\n",
    "            estimate_values[f'{environment_object}'].append(estimate.value)\n",
    "\n",
    "        # IV. Refute the obtained estimate using multiple robustness checks.\n",
    "        method_names = ['random_common_cause', 'placebo_treatment_refuter', 'data_subset_refuter', 'bootstrap_refuter']\n",
    "        for environment_object in environment_objects:\n",
    "            for method_name in method_names:\n",
    "                refute_results = models[f'{environment_object}'][0].refute_estimate(estimands[f'{environment_object}'][0],\n",
    "                                                                                    estimates[f'{environment_object}'][0],\n",
    "                                                                                    method_name=method_name)\n",
    "                refutation_estimates[f'{environment_object}'].append(refute_results)\n",
    "\n",
    "        return models, estimands, estimates, estimate_values, refutation_estimates\n",
    "\n",
    "    def object_to_reward(self):\n",
    "        \"\"\"This method computes the causal estimate from the objects to the rewards not considering the common\n",
    "        causes.\"\"\"\n",
    "\n",
    "        # Necessary dictionaries.\n",
    "        models = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimands = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimate_values = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        refutation_estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        environment_objects = ['Breeze', 'Gold', 'Pit', 'Stench', 'Wumpus']\n",
    "\n",
    "        # I. Create a causal model from the data and given graph.\n",
    "        for environment_object in environment_objects:\n",
    "            graph = f'graph[directed 1node[ id \"Reward\" label \"Reward\"]node[ id \"Next State At {environment_object}\"' \\\n",
    "                f' label \"Next State At {environment_object}\"]edge[source \"Next State At {environment_object}\" target' \\\n",
    "                f' \"Reward\"]]'\n",
    "            model = CausalModel(data=self.data, treatment=[f'Next State At {environment_object}'], outcome=['Reward'],\n",
    "                                graph=graph)\n",
    "            models[f'{environment_object}'].append(model)\n",
    "\n",
    "        # II. Identify causal effect and return target estimands.\n",
    "        for environment_object in environment_objects:\n",
    "            estimands[f'{environment_object}'].append(models[f'{environment_object}'][0].identify_effect())\n",
    "\n",
    "        # III. Estimate the target estimand using a statistical method.\n",
    "        for environment_object in environment_objects:\n",
    "            estimate = models[f'{environment_object}'][0].estimate_effect(estimands[f'{environment_object}'][0],\n",
    "                                                                          method_name=\"backdoor.linear_regression\",\n",
    "                                                                          test_significance=True, control_value=0,\n",
    "                                                                          treatment_value=1)\n",
    "            estimates[f'{environment_object}'].append(estimate)\n",
    "            estimate_values[f'{environment_object}'].append(estimate.value)\n",
    "\n",
    "        # IV. Refute the obtained estimate using multiple robustness checks.\n",
    "        method_names = ['random_common_cause', 'placebo_treatment_refuter', 'data_subset_refuter', 'bootstrap_refuter']\n",
    "        for environment_object in environment_objects:\n",
    "            for method_name in method_names:\n",
    "                refute_results = models[f'{environment_object}'][0].refute_estimate(estimands[f'{environment_object}'][0],\n",
    "                                                                                    estimates[f'{environment_object}'][0],\n",
    "                                                                                    method_name=method_name)\n",
    "                refutation_estimates[f'{environment_object}'].append(refute_results)\n",
    "\n",
    "        return models, estimands, estimates, estimate_values, refutation_estimates\n",
    "\n",
    "    def object_to_reward_common_causes(self):\n",
    "        \"\"\"This method computes the causal estimate from the objects to the rewards considering the common causes.\"\"\"\n",
    "\n",
    "        # Necessary dictionaries.\n",
    "        models = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimands = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimate_values = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        refutation_estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        environment_objects = ['Breeze', 'Gold', 'Pit', 'Stench', 'Wumpus']\n",
    "\n",
    "        # I. Create a causal model from the data and given graph. \n",
    "        for environment_object in environment_objects:\n",
    "            common_causes = ['Next State At Gold', 'Next State At Wumpus', 'Next State At Breeze',\n",
    "                                 'Next State At Stench', 'Next State At Pit']\n",
    "            common_causes.remove(f'Next State At {environment_object}')\n",
    "            model = CausalModel(data=self.data, treatment=[f'Next State At {environment_object}'], outcome=['Reward'],\n",
    "                                common_causes=common_causes)\n",
    "            models[f'{environment_object}'].append(model)\n",
    "        \n",
    "        # II. Identify causal effect and return target estimands.\n",
    "        for environment_object in environment_objects:\n",
    "            estimands[f'{environment_object}'].append(models[f'{environment_object}'][0].identify_effect())\n",
    "\n",
    "        # III. Estimate the target estimand using a statistical method.\n",
    "        for environment_object in environment_objects:\n",
    "            estimate = models[f'{environment_object}'][0].estimate_effect(estimands[f'{environment_object}'][0],\n",
    "                                                                       method_name=\"backdoor.linear_regression\",\n",
    "                                                 test_significance=True, control_value=0, treatment_value=1)\n",
    "            estimates[f'{environment_object}'].append(estimate)\n",
    "            estimate_values[f'{environment_object}'].append(estimate.value)\n",
    "\n",
    "        # IV. Refute the obtained estimate using multiple robustness checks.\n",
    "        method_names = ['random_common_cause', 'placebo_treatment_refuter', 'data_subset_refuter', 'bootstrap_refuter']\n",
    "        for environment_object in environment_objects:\n",
    "            for method_name in method_names:\n",
    "                refute_results = models[f'{environment_object}'][0].refute_estimate(estimands[f'{environment_object}'][0], estimates[f'{environment_object}'][0], method_name=method_name)\n",
    "                refutation_estimates[f'{environment_object}'].append(refute_results)\n",
    "   \n",
    "        return models, estimands, estimates, estimate_values, refutation_estimates\n",
    "    \n",
    "    def object_to_done(self):\n",
    "        \"\"\"This method computes the causal estimate from the objects to the rewards considering the common causes.\"\"\"\n",
    "\n",
    "        # Necessary dictionaries.\n",
    "        models = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimands = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimate_values = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        refutation_estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        environment_objects = ['Breeze', 'Gold', 'Pit', 'Stench', 'Wumpus']\n",
    "\n",
    "        # I. Create a causal model from the data and given graph.\n",
    "        for environment_object in environment_objects:\n",
    "            common_causes = ['Next State At Gold', 'Next State At Wumpus', 'Next State At Breeze',\n",
    "                                 'Next State At Stench', 'Next State At Pit']\n",
    "            common_causes.remove(f'Next State At {environment_object}')\n",
    "            model = CausalModel(data=self.data, outcome=['Done'], treatment=[f'Next State At {environment_object}'],\n",
    "                                common_causes=common_causes)\n",
    "            models[f'{environment_object}'].append(model)\n",
    "\n",
    "        # II. Identify causal effect and return target estimands.\n",
    "        for environment_object in environment_objects:\n",
    "            estimands[f'{environment_object}'].append(models[f'{environment_object}'][0].identify_effect())\n",
    "\n",
    "        # III. Estimate the target estimand using a statistical method.\n",
    "        for environment_object in environment_objects:\n",
    "            estimate = models[f'{environment_object}'][0].estimate_effect(estimands[f'{environment_object}'][0],\n",
    "                                                                       method_name=\"backdoor.linear_regression\",\n",
    "                                                 test_significance=True, control_value=0, treatment_value=1)\n",
    "            estimates[f'{environment_object}'].append(estimate)\n",
    "            estimate_values[f'{environment_object}'].append(estimate.value)\n",
    "\n",
    "        # IV. Refute the obtained estimate using multiple robustness checks.\n",
    "        method_names = ['random_common_cause', 'placebo_treatment_refuter', 'data_subset_refuter', 'bootstrap_refuter']\n",
    "        for environment_object in environment_objects:\n",
    "            for method_name in method_names:\n",
    "                refute_results = models[f'{environment_object}'][0].refute_estimate(estimands[f'{environment_object}'][0], estimates[f'{environment_object}'][0], method_name=method_name)\n",
    "                refutation_estimates[f'{environment_object}'].append(refute_results)\n",
    "   \n",
    "        return models, estimands, estimates, estimate_values, refutation_estimates\n",
    "    \n",
    "    def state_to_next_state(self):\n",
    "        \"\"\"This method computes the causal estimate from the state to the next state considering the actions as effect\n",
    "        modifiers.\"\"\"\n",
    "\n",
    "        # Necessary dictionaries.\n",
    "        models = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimands = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimates = {'Breeze': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Gold': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Pit': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Stench': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Wumpus': {'Left': [], 'Right': [], 'Up': [], 'Down': []}}\n",
    "        estimate_values = {'Breeze': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Gold': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Pit': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Stench': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Wumpus': {'Left': [], 'Right': [], 'Up': [], 'Down': []}}\n",
    "        refutation_estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        actions = ['Left', 'Right', 'Up', 'Down']\n",
    "        positions = ['Left', 'Right', 'Up', 'Down']\n",
    "        environment_objects = ['Breeze', 'Gold', 'Pit', 'Stench', 'Wumpus']\n",
    "\n",
    "        # I. Create a causal model from the data and given graph.\n",
    "        for environment_object in environment_objects:\n",
    "            for action in actions:\n",
    "                graph = f'graph[directed 1node[ id \"Next State At {environment_object}\" label \"Next State At ' \\\n",
    "                        f'{environment_object}\"]node[ id \"State {action} {environment_object}\" label \"State ' \\\n",
    "                        f'{action} {environment_object}\"]edge[source \"State {action} {environment_object}\" target ' \\\n",
    "                        f'\"Next State At {environment_object}\"]]'\n",
    "                common_causes = ['State At Breeze', 'State At Gold', 'State At Pit', 'State At Stench', 'State At Wumpus',\n",
    "                 'State Left Breeze', 'State Right Breeze', 'State Up Breeze', 'State Down Breeze', 'State Left Gold',\n",
    "                 'State Right Gold', 'State Up Gold', 'State Down Gold', 'State Left Pit', 'State Right Pit',\n",
    "                 'State Up Pit', 'State Down Pit', 'State Left Stench', 'State Right Stench', 'State Up Stench',\n",
    "                 'State Down Stench', 'State Left Wumpus', 'State Right Wumpus', 'State Up Wumpus', 'State Down Wumpus']\n",
    "\n",
    "                common_causes.remove(f'State {action} {environment_object}')\n",
    "                current_object = environment_object\n",
    "                for environment_object_ in environment_objects:\n",
    "                    if current_object != environment_object_:\n",
    "                        common_causes.remove(f'State At {environment_object_}')\n",
    "                    for action_ in actions:\n",
    "                        if current_object != environment_object_:\n",
    "                            common_causes.remove(f'State {action_} {environment_object_}')\n",
    "\n",
    "                model = CausalModel(data=self.data, treatment=[f'State {action} {environment_object}'],\n",
    "                                    outcome=[f'Next State At {environment_object}'], common_causes=common_causes,\n",
    "                                    effect_modifiers=['Action Left', 'Action Right', 'Action Up', 'Action Down'])\n",
    "                models[f'{environment_object}'].append(model)\n",
    "\n",
    "        # II. Identify causal effect and return target estimands.\n",
    "        for environment_object in environment_objects:\n",
    "            for i in range(len(actions)):\n",
    "                estimands[f'{environment_object}'].append(\n",
    "                    models[f'{environment_object}'][i].identify_effect(proceed_when_unidentifiable=True))\n",
    "\n",
    "        # III. Estimate the target estimand using a statistical method.\n",
    "        for environment_object in environment_objects:\n",
    "            for i, position in enumerate(positions):\n",
    "                for action in actions:\n",
    "                    dml_estimate = models[f'{environment_object}'][i].estimate_effect(\n",
    "                        estimands[f'{environment_object}'][i], method_name=\"backdoor.econml.dml.DML\",\n",
    "                        control_value=0, treatment_value=1, target_units=lambda df: df[f\"Action {action}\"] > 0,\n",
    "                        confidence_intervals=False, method_params={\n",
    "                            \"init_params\": {'model_y': GradientBoostingRegressor(),\n",
    "                                            'model_t': GradientBoostingRegressor(),\n",
    "                                            \"model_final\": LassoCV(fit_intercept=False),\n",
    "                                            'featurizer': PolynomialFeatures(degree=1, include_bias=True)},\n",
    "                            \"fit_params\": {}})\n",
    "                    estimates[f'{environment_object}'][f'{position}'].append(dml_estimate)\n",
    "                    estimate_values[f'{environment_object}'][f'{position}'].append(dml_estimate.value)\n",
    "\n",
    "        # # IV. Refute the obtained estimate using multiple robustness checks.\n",
    "        # method_names = ['random_common_cause', 'placebo_treatment_refuter', 'data_subset_refuter', 'bootstrap_refuter']\n",
    "        # for environment_object in environment_objects:\n",
    "        #     print('\\nEnvironment Object:', environment_object)\n",
    "        #     for method_name in method_names:\n",
    "        #         print('Method Name:', method_name)\n",
    "        #         index = 0\n",
    "        #         for i, action in enumerate(actions):\n",
    "        #             print('I', i, 'Action:', action)\n",
    "        #             while index < 16:\n",
    "        #                 print('Index:', index)\n",
    "        #                 refute_results = models[f'{environment_object}'][i].refute_estimate(\n",
    "        #                     estimands[f'{environment_object}'][i], estimates[f'{environment_object}'][index],\n",
    "        #                     method_name=method_name)\n",
    "        #                 refutation_estimates[f'{environment_object}'].append(refute_results)\n",
    "        #                 print('REFUTE RESULTS', refute_results)\n",
    "        #                 index += 1\n",
    "        #                 if index % 4 == 0:\n",
    "        #                     break\n",
    " \n",
    "        return models, estimands, estimates, estimate_values, refutation_estimates\n",
    "\n",
    "    def state_to_reward(self):\n",
    "        \"\"\"This method computes the causal estimate from the state to the rewards considering the actions as effect\n",
    "        modifiers.\"\"\"\n",
    "\n",
    "        # Necessary dictionaries.\n",
    "        models = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimands = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        estimates = {'Breeze': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Gold': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Pit': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Stench': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Wumpus': {'Left': [], 'Right': [], 'Up': [], 'Down': []}}\n",
    "        estimate_values = {'Breeze': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Gold': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Pit': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Stench': {'Left': [], 'Right': [], 'Up': [], 'Down': []}, 'Wumpus': {'Left': [], 'Right': [], 'Up': [], 'Down': []}}\n",
    "        refutation_estimates = {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
    "        actions = ['Left', 'Right', 'Up', 'Down']\n",
    "        positions = ['Left', 'Right', 'Up', 'Down']\n",
    "        environment_objects = ['Breeze', 'Gold', 'Pit', 'Stench', 'Wumpus']\n",
    "\n",
    "        # I. Create a causal model from the data and given graph.\n",
    "        for environment_object in environment_objects:\n",
    "            for action in actions:\n",
    "                common_causes = ['Next State At Gold', 'Next State At Wumpus', 'Next State At Breeze',\n",
    "                                 'Next State At Stench', 'Next State At Pit']\n",
    "                common_causes.remove(f'Next State At {environment_object}')\n",
    "                model = CausalModel(data=self.data, treatment=[f'State {action} {environment_object}'],\n",
    "                                    outcome=['Reward'], common_causes=common_causes,\n",
    "                                    effect_modifiers=['Action Left', 'Action Right', 'Action Up', 'Action Down'])\n",
    "                models[f'{environment_object}'].append(model)\n",
    "\n",
    "        # II. Identify causal effect and return target estimands\n",
    "        for environment_object in environment_objects:\n",
    "            for i in range(len(actions)):\n",
    "                estimands[f'{environment_object}'].append(\n",
    "                    models[f'{environment_object}'][i].identify_effect(proceed_when_unidentifiable=True))\n",
    "\n",
    "        # III. Estimate the target estimand using a statistical method.\n",
    "        for environment_object in environment_objects:\n",
    "            for i, position in enumerate(positions):\n",
    "                for action in actions:\n",
    "                    dml_estimate = models[f'{environment_object}'][i].estimate_effect(\n",
    "                        estimands[f'{environment_object}'][i], method_name=\"backdoor.econml.dml.DML\",\n",
    "                        control_value=0, treatment_value=1, target_units=lambda df: df[f\"Action {action}\"] > 0,\n",
    "                        confidence_intervals=False, method_params={\n",
    "                            \"init_params\": {'model_y': GradientBoostingRegressor(),\n",
    "                                            'model_t': GradientBoostingRegressor(),\n",
    "                                            \"model_final\": LassoCV(fit_intercept=False),\n",
    "                                            'featurizer': PolynomialFeatures(degree=1, include_bias=False)},\n",
    "                            \"fit_params\": {}})\n",
    "                    estimates[f'{environment_object}'][f'{position}'].append(dml_estimate)\n",
    "                    estimate_values[f'{environment_object}'][f'{position}'].append(dml_estimate.value)\n",
    "\n",
    "        # # IV. Refute the obtained estimate using multiple robustness checks.\n",
    "        # method_names = ['random_common_cause', 'placebo_treatment_refuter', 'data_subset_refuter', 'bootstrap_refuter']\n",
    "        # for environment_object in environment_objects:\n",
    "        #     for method_name in method_names:\n",
    "        #         index = 0\n",
    "        #         for i, action in enumerate(actions):\n",
    "        #             while index < 16:\n",
    "        #                 refute_results = models[f'{environment_object}'][i].refute_estimate(\n",
    "        #                     estimands[f'{environment_object}'][i], estimates[f'{environment_object}'][index],\n",
    "        #                     method_name=method_name)\n",
    "        #                 refutation_estimates[f'{environment_object}'].append(refute_results)\n",
    "        #                 index += 1\n",
    "        #                 if index % 4 == 0:\n",
    "        #                     break\n",
    "\n",
    "        return models, estimands, estimates, estimate_values, refutation_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5EkqXVb2snlb"
   },
   "outputs": [],
   "source": [
    "# Instantiating the Wumpus World environment.\n",
    "wumpus_world_environment_training = WumpusWorldEnvironment(environment_type='training')\n",
    "wumpus_world_environment_testing = WumpusWorldEnvironment(environment_type='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating data through random exploration:\n",
      "\n",
      "Causal estimate from reward to objects:\n",
      "Estimands:\n",
      " {'Breeze': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x0000029222D943A0>], 'Gold': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292B90F8F40>], 'Pit': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292B91B5070>], 'Stench': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292B9118940>], 'Wumpus': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292B9118C10>]}\n",
      "Estimates:\n",
      " {'Breeze': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BBE8C4C0>], 'Gold': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BBEC44F0>], 'Pit': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BBECCF70>], 'Stench': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BBEC4A00>], 'Wumpus': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BBEC4C40>]}\n",
      "Estimate Values:\n",
      " {'Breeze': [0.0009211740925840761], 'Gold': [0.0009472382093394629], 'Pit': [-0.0007152873477608429], 'Stench': [-4.226001108352528e-06], 'Wumpus': [-0.00016997423272406206]}\n",
      "Refutation Estimates:\n",
      ": {'Breeze': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BBECCCA0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBE8CE50>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC4004C0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC306970>], 'Gold': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC17A4C0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC173730>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC3C5C10>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC10C850>], 'Pit': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC302A30>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC409220>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC1A0C70>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF1C130>], 'Stench': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC31F7F0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF353A0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC1C3E80>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC40F4F0>], 'Wumpus': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFCD6D0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF83FA0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC199D90>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF4ABE0>]}\n",
      "\n",
      "Causal estimate  from the object to rewards not considering the common causes:\n",
      "Estimands:\n",
      " {'Breeze': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC1AC4F0>], 'Gold': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC175460>], 'Pit': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC175D30>], 'Stench': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BBF67E80>], 'Wumpus': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BBFA3100>]}\n",
      "Estimates:\n",
      " {'Breeze': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC07F820>], 'Gold': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC192F40>], 'Pit': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC3CB040>], 'Stench': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC1D2700>], 'Wumpus': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC1EDF70>]}\n",
      "Estimate Values:\n",
      " {'Breeze': [29.19172927352675], 'Gold': [1006.5601836931653], 'Pit': [-56.46372535872319], 'Stench': [-0.5593840394758617], 'Wumpus': [-101.80466661265487]}\n",
      "Refutation Estimates:\n",
      ": {'Breeze': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFABF10>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC0B3940>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC119A00>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC008C40>], 'Gold': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC40F700>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC188B20>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC231A60>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC24C5E0>], 'Pit': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC119940>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC08DC70>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF0CD30>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BD8C0FA0>], 'Stench': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC2EA550>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BD8F33A0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFCD760>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFCD970>], 'Wumpus': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC052850>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF76460>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC30BA60>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC2803D0>]}\n",
      "\n",
      "Causal estimate from the object to the rewards considering the common causes:\n",
      "Estimands:\n",
      " {'Breeze': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC213FA0>], 'Gold': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC280B20>], 'Pit': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC4000A0>], 'Stench': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC4BB2E0>], 'Wumpus': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC42AF70>]}\n",
      "Estimates:\n",
      " {'Breeze': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC188D90>], 'Gold': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC04B640>], 'Pit': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC38A370>], 'Stench': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC2EA7C0>], 'Wumpus': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BBEEC730>]}\n",
      "Estimate Values:\n",
      " {'Breeze': [-1.0769163338864018e-13], 'Gold': [999.9999999999649], 'Pit': [-49.9999999999997], 'Stench': [4.624078897563777e-13], 'Wumpus': [-99.99999999999929]}\n",
      "Refutation Estimates:\n",
      ": {'Breeze': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC333E80>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC4ABDC0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC656BB0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC477F70>], 'Gold': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC2708E0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC5977C0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC553910>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC5B1BE0>], 'Pit': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC571D90>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC374640>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC0B3D60>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC48ECD0>], 'Stench': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC023B50>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC04B7F0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC07FFA0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBF76DC0>], 'Wumpus': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BD8C01C0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC052790>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC4D88E0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC629820>]}\n",
      "\n",
      "Causal estimate from the object to the terminal condition considering the common causes:\n",
      "Estimands:\n",
      " {'Breeze': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC237A60>], 'Gold': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC1D15B0>], 'Pit': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC2FB3D0>], 'Stench': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BBFC1490>], 'Wumpus': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BBF605B0>]}\n",
      "Estimates:\n",
      " {'Breeze': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC1AA610>], 'Gold': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC409400>], 'Pit': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC077AF0>], 'Stench': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC0696D0>], 'Wumpus': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC1A6940>]}\n",
      "Estimate Values:\n",
      " {'Breeze': [-9.381384558082573e-15], 'Gold': [0.9999999999999635], 'Pit': [0.9999999999999966], 'Stench': [-2.3592239273284576e-16], 'Wumpus': [0.9999999999999902]}\n",
      "Refutation Estimates:\n",
      ": {'Breeze': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC323C10>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC30B460>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC2873A0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC09C6D0>], 'Gold': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC42AEB0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFFB2B0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC1F5D60>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC04BB50>], 'Pit': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC25B4F0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC462880>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFD3AC0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC10C4C0>], 'Stench': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BBFCDE50>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC5280A0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC007760>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC21AEE0>], 'Wumpus': [<dowhy.causal_refuter.CausalRefutation object at 0x00000292BC1A09D0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC052880>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BD89C2B0>, <dowhy.causal_refuter.CausalRefutation object at 0x00000292BC656490>]}\n",
      "\n",
      "Causal estimate from the state to the next state:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimands:\n",
      " {'Breeze': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD80C970>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC5A6A00>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD89C220>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC3EC160>], 'Gold': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD81DDF0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC58EEB0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC1AA9A0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC130790>], 'Pit': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BF05F790>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC202EB0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC199130>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC422190>], 'Stench': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC422550>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD80CD60>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC5F93D0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BC37D1C0>], 'Wumpus': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD80CA90>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD80CC70>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD80CCD0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292BD848B80>]}\n",
      "Estimates:\n",
      " {'Breeze': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC070820>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292BC254C40>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292BC0DD430>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292BC280F10>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292BC138550>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C4B4FE20>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C4B585E0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C4B987C0>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C4BDD880>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C4C0DB20>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C4C3A1F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C4C68E20>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C80BC340>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C80F5520>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C810B760>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C8139850>]}, 'Gold': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C816A190>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C81962B0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C81E2220>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C81FE400>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C8225670>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C8254850>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C82839D0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C82B10A0>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C92AEF40>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C828F1F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C930E3D0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9315610>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C936E7F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C939C970>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C93C9280>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C94031C0>]}, 'Pit': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C9435280>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9459490>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C94876D0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C94B9850>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C94E5160>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9511340>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9546160>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C958D250>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C95A8430>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C95F4EE0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9601A30>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C962D100>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C965C2E0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C968AF40>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C96CC1F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C96F0460>]}, 'Stench': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C971B850>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9747B20>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C97761F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C97A4E20>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C97E9100>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C980D2E0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9847760>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C9863850>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C9893AF0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C98BF640>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C98EEE20>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C99260D0>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292C996D4F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292C998A730>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CA9C4F40>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CA9CFA90>]}, 'Wumpus': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CA9F9340>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAA3C1F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAA72400>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAA90640>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CAAB5820>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAAE39A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAB12070>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAB3F250>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CAB771C0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CABB73A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CABE85E0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CABFB7C0>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CAC2A880>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAC59B20>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CACA2250>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAC3B160>]}}\n",
      "Estimate Values:\n",
      " {'Breeze': {'Left': [0.9933726435135993, 0.0, 0.0012068012428506675, 0.001105247596570219], 'Right': [0.0, 1.0003167200765528, 0.0, -2.1298714125732397e-05], 'Up': [0.0, 0.0, 0.999046587687463, 0.0], 'Down': [5.247949754243608e-05, 0.00034863611404790105, 0.0, 0.9976992994384203]}, 'Gold': {'Left': [0.0, 0.0, 0.0, 0.0], 'Right': [0.0, 0.0, 0.0, 0.0], 'Up': [1.3691023713525845e-05, 1.3738830057171007e-05, 0.9990000003957179, 1.3776796582753139e-05], 'Down': [0.0, 0.0, 0.0, 0.0]}, 'Pit': {'Left': [0.9989764615119442, 0.0, 0.0003348467948714945, 0.00012187501391750366], 'Right': [1.5437338699750763e-05, 0.9990456152633213, 6.252670845622362e-05, 0.0], 'Up': [0.0004106532682009589, 0.0, 0.9990755341984746, 0.00012364720923461269], 'Down': [0.0, 0.0002370611177711193, 0.0, 0.9991649753205062]}, 'Stench': {'Left': [0.9990011951942191, 0.0, 3.345561642281329e-05, 5.212537158758218e-05], 'Right': [1.6108998006862775e-05, 0.9990242088039624, 0.0, 3.410259946500642e-05], 'Up': [0.0, 0.0, 0.9982489464238706, 0.0], 'Down': [0.0005014168101523655, 0.00018511449009403358, 9.035619644944586e-05, 0.9990551757487937]}, 'Wumpus': {'Left': [0.9990097856888893, 0.0, 6.980279502569156e-06, 0.0], 'Right': [8.870026133365344e-06, 0.9988813143826571, 6.691749458040203e-05, 9.445974617209747e-05], 'Up': [2.576736250169549e-05, 0.0, 0.9990093860897619, 2.307014947005072e-05], 'Down': [5.9434302759962954e-05, 5.755753691320511e-06, 1.8410462874536367e-05, 0.9990492883606734]}}\n",
      "Refutation Estimates:\n",
      ": {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n",
      "\n",
      "Causal estimate from the state to the reward:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimands:\n",
      " {'Breeze': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAC3B190>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAC3B250>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACDEC40>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACDE9D0>], 'Gold': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACDECA0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACDEBB0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACDEE80>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACDEFA0>], 'Pit': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE50D0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE51C0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE52B0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE53A0>], 'Stench': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE5490>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE5580>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAD311F0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CACE5760>], 'Wumpus': [<dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAD31FA0>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAD31A00>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAD31370>, <dowhy.causal_identifier.IdentifiedEstimand object at 0x00000292CAD31D60>]}\n",
      "Estimates:\n",
      " {'Breeze': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CAD37310>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAD67A90>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CACDEAF0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CADAE430>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CADFD670>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAE24850>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CAE549D0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBE71C10>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CBE71BB0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBED8430>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBF00670>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBF2E850>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CBF5C9D0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBF8AC10>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBF8ABB0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CBFEA430>]}, 'Gold': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CC017670>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CC047850>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CC0749D0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CC0A3C10>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CC0A3BB0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CC103430>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CC14B670>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD155EE0>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CD15DA30>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD18B310>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CC128C10>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD1A0190>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CC0D2700>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD22D640>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD24E820>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD2799A0>]}, 'Pit': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CD2A7070>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD2DF1F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD309220>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD339400>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CD37F640>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD394820>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD3C39A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD3EF2B0>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CD4391F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD44F1C0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD4803A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD4B55E0>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CD4F8760>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD50A070>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD537250>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CD568190>]}, 'Stench': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CD5711C0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE5963A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE5AE5E0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE5F49A0>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CE623070>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE650250>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE688190>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE6C6160>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CE6FB3A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE7187C0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE73D880>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE76CB20>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CE79A1F0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE7CBE20>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE80F100>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE82E520>]}, 'Wumpus': {'Left': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CE859760>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE888850>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE8B7AC0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE8E4190>], 'Right': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CE913DC0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE94D2E0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE8FF4C0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CE9BA700>], 'Up': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CE9F6F10>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CEA00A60>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CEA2C130>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CEA67190>], 'Down': [<dowhy.causal_estimator.CausalEstimate object at 0x00000292CEA8E280>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CFA90460>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CFABD6A0>, <dowhy.causal_estimator.CausalEstimate object at 0x00000292CFAEA880>]}}\n",
      "Estimate Values:\n",
      " {'Breeze': {'Left': [1.2569065164845485e-06, 5.782631578417245e-06, 9.875922015743708e-06, 3.7577800216547693e-06], 'Right': [3.915531704891053e-06, 8.898577344665319e-07, 5.857080378932991e-06, -7.157401563802034e-07], 'Up': [-5.807322603649568e-08, 2.044229979667619e-07, 2.9084411791817847e-06, -3.928091030462511e-08], 'Down': [-5.609403479619168e-07, 6.1646872816701085e-06, -9.211796732196216e-06, 8.846248135904573e-07]}, 'Gold': {'Left': [0.0, 0.0, 0.0, 0.0], 'Right': [0.0, 0.0, 0.0, 0.0], 'Up': [0.05870975545474187, 0.04542435477836861, 977.6966160926979, 0.05218170983937387], 'Down': [0.0, 0.0, 0.0, 0.0]}, 'Pit': {'Left': [-49.591076250269104, 4.785904836108904, 4.801236758895719, 4.329753798222626], 'Right': [-0.7103310273699542, -49.95711222169559, 12.747039260073736, 2.316115212786262], 'Up': [1.2357772250302312, 11.755035911543105, -49.93798792636542, -10.102626411503826], 'Down': [4.892604813416391, 4.326833200729383, -14.35803012364099, -49.93633252701597]}, 'Stench': {'Left': [0.0, 4.996865502234745e-06, 2.2457624181565658e-05, -3.6150003269343267e-06], 'Right': [-1.6409220696047694e-06, 0.0, 0.0, 0.0], 'Up': [-2.2521421118087946e-07, -7.138821311648845e-07, 3.6164028398137245e-07, 0.0], 'Down': [-8.664908883431418e-06, -2.4076474051496674e-05, 0.10143303083087002, 2.3747289906293763e-05]}, 'Wumpus': {'Left': [-99.8994981020517, 2.4907125106356673, 3.0703516315653983, 1.1231312549369221], 'Right': [3.5742249979790515, -99.8769534820991, 0.0, -0.006161196839232635], 'Up': [0.025255241177364256, 2.4609371592067335, -99.89013133589546, 0.9829852282177497], 'Down': [0.0, 2.262319246306853, 0.0, -99.8838389565615]}}\n",
      "Refutation Estimates:\n",
      ": {'Breeze': [], 'Gold': [], 'Pit': [], 'Stench': [], 'Wumpus': []}\n"
     ]
    }
   ],
   "source": [
    "generate_causal_map = GenerateCausalMap(wumpus_world_environment_training)\n",
    "\n",
    "print('\\nGenerating data through random exploration:')\n",
    "generate_causal_map.generate_random_data()\n",
    "\n",
    "print('\\nCausal estimate from reward to objects:')\n",
    "reward_to_object_models, reward_to_object_estimands, reward_to_object_estimates, reward_to_object_estimate_values, reward_to_object_refutation_estimates = generate_causal_map.reward_to_object()\n",
    "print('Estimands:\\n', reward_to_object_estimands)                   \n",
    "print('Estimates:\\n', reward_to_object_estimates)\n",
    "print('Estimate Values:\\n', reward_to_object_estimate_values)\n",
    "print('Refutation Estimates:\\n:', reward_to_object_refutation_estimates)\n",
    "\n",
    "print('\\nCausal estimate  from the object to rewards not considering the common causes:')\n",
    "object_to_reward_models, object_to_reward_estimands, object_to_reward_estimates, object_to_reward_estimate_values, object_to_reward_refutation_estimates = generate_causal_map.object_to_reward()\n",
    "print('Estimands:\\n', object_to_reward_estimands)\n",
    "print('Estimates:\\n', object_to_reward_estimates)\n",
    "print('Estimate Values:\\n', object_to_reward_estimate_values)\n",
    "print('Refutation Estimates:\\n:', object_to_reward_refutation_estimates)\n",
    "\n",
    "print('\\nCausal estimate from the object to the rewards considering the common causes:')\n",
    "object_to_reward_common_causes_models, object_to_reward_common_causes_estimands, object_to_reward_common_causes_estimates, object_to_reward_common_causes_estimate_values, object_to_reward_common_causes_refutation_estimates = generate_causal_map.object_to_reward_common_causes()\n",
    "print('Estimands:\\n', object_to_reward_common_causes_estimands)\n",
    "print('Estimates:\\n', object_to_reward_common_causes_estimates)\n",
    "print('Estimate Values:\\n', object_to_reward_common_causes_estimate_values)\n",
    "print('Refutation Estimates:\\n:', object_to_reward_common_causes_refutation_estimates)\n",
    "\n",
    "print('\\nCausal estimate from the object to the terminal condition considering the common causes:')\n",
    "object_to_done_models, object_to_done_estimands, object_to_done_estimates, object_to_done_estimate_values, object_to_done_refutation_estimates = generate_causal_map.object_to_done()\n",
    "print('Estimands:\\n', object_to_done_estimands)\n",
    "print('Estimates:\\n', object_to_done_estimates)\n",
    "print('Estimate Values:\\n', object_to_done_estimate_values)\n",
    "print('Refutation Estimates:\\n:', object_to_done_refutation_estimates)\n",
    "\n",
    "print('\\nCausal estimate from the state to the next state:')\n",
    "state_to_next_state_models, state_to_next_state_estimands, state_to_next_state_estimates, state_to_next_state_estimate_values, state_to_next_state_refutation_estimates = generate_causal_map.state_to_next_state()\n",
    "print('Estimands:\\n', state_to_next_state_estimands)\n",
    "print('Estimates:\\n', state_to_next_state_estimates)\n",
    "print('Estimate Values:\\n', state_to_next_state_estimate_values)\n",
    "print('Refutation Estimates:\\n:', state_to_next_state_refutation_estimates)\n",
    "\n",
    "print('\\nCausal estimate from the state to the reward:')\n",
    "state_to_reward_models, state_to_reward_estimands, state_to_reward_estimates, state_to_reward_estimate_values, state_to_reward_refutation_estimates = generate_causal_map.state_to_reward()\n",
    "print('Estimands:\\n', state_to_reward_estimands)\n",
    "print('Estimates:\\n', state_to_reward_estimates)\n",
    "print('Estimate Values:\\n', state_to_reward_estimate_values)\n",
    "print('Refutation Estimates:\\n:', state_to_reward_refutation_estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class Policy:\n",
    "    \"\"\"This class learns the policy.\"\"\"\n",
    "\n",
    "    def __init__(self, test_environment):\n",
    "        self.test_environment = test_environment\n",
    "        \n",
    "    def identify_primary_objective(self):\n",
    "        \"\"\"This method identifies the primary objective of the agent.\"\"\"\n",
    "        primary_objective = None\n",
    "        primary_objective_reward = None\n",
    "        for reward_value, done_estimate in zip(object_to_reward_common_causes_estimate_values, object_to_done_estimates):\n",
    "            if not primary_objective:\n",
    "                if object_to_done_estimate_values[f'{done_estimate}'][0]:\n",
    "                    primary_objective = reward_value\n",
    "                    primary_objective_reward = object_to_reward_common_causes_estimate_values[f'{reward_value}'][0]\n",
    "            else:\n",
    "                if object_to_done_estimates[f'{done_estimate}'][0]:\n",
    "                    if primary_objective_reward < object_to_reward_common_causes_estimate_values[f'{reward_value}'][0]:\n",
    "                        primary_objective = reward_value\n",
    "                        primary_objective_reward = object_to_reward_common_causes_estimate_values[f'{reward_value}'][0]\n",
    "        \n",
    "        return primary_objective, primary_objective_reward\n",
    "    \n",
    "    def identify_secondary_objectives(self):\n",
    "        \"\"\"This method identifies the secondary objectives of the agent.\"\"\"\n",
    "        \n",
    "        secondary_objectives = []\n",
    "        secondary_objectives_rewards = []\n",
    "        for reward_value, done_estimate in zip(object_to_reward_common_causes_estimate_values, object_to_done_estimate_values):\n",
    "              if (-0.1 < (object_to_done_estimate_values[f'{done_estimate}'][0]) < 0.1) and ((object_to_reward_common_causes_estimate_values[f'{reward_value}'][0] + primary_objective_reward) > primary_objective_reward + 0.001 * primary_objective_reward):\n",
    "                    secondary_objectives.append(reward_value)\n",
    "                    secondary_objectives_rewards.append(object_to_reward_common_causes_estimate_values[f'{reward_value}'][0])\n",
    "        \n",
    "        return secondary_objectives, secondary_objectives_rewards\n",
    "    \n",
    "    def identify_objects_to_avoid(self):\n",
    "        objects_to_avoid = []\n",
    "        objects_to_avoid_rewards = []\n",
    "\n",
    "        for reward_value, done_estimate in zip(object_to_reward_common_causes_estimate_values, object_to_done_estimate_values):\n",
    "              if (0.9 < (object_to_done_estimate_values[f'{done_estimate}'][0]) < 1.1) and ((object_to_reward_common_causes_estimate_values[f'{reward_value}'][0] + primary_objective_reward) < primary_objective_reward):\n",
    "                    objects_to_avoid.append(reward_value)\n",
    "                    objects_to_avoid_rewards.append(object_to_reward_common_causes_estimate_values[f'{reward_value}'][0])\n",
    "        \n",
    "        return objects_to_avoid, objects_to_avoid_rewards\n",
    "    \n",
    "    def identify_situations_previously_unencountered(self):\n",
    "        \n",
    "        situations_unencountered = []\n",
    "        for column_name in generate_causal_map.data.columns:\n",
    "            if all(generate_causal_map.data[column_name] == False):\n",
    "                situations_unencountered.append(column_name)\n",
    "        return situations_unencountered\n",
    "    \n",
    "    def generalize_for_unencountered_situations(self):\n",
    "        \n",
    "        testdf = pd.DataFrame({'Object': [], 'Position': [], 'Action Estimates': []})\n",
    "        testdf['Object'] = list(chain.from_iterable([key for i in range(4)] for key in state_to_next_state_estimate_values.keys()))\n",
    "        testdf['Position'] = list(chain.from_iterable([pos_key for pos_key in state_to_next_state_estimate_values[key].keys()] for key in state_to_next_state_estimate_values.keys()))\n",
    "        testdf['Action Estimates'] = list(chain.from_iterable([state_to_next_state_estimate_values[key][pos_key] for pos_key in state_to_next_state_estimate_values[key].keys()] for key in state_to_next_state_estimate_values.keys()))\n",
    "        indexes = testdf[ (testdf['Object'] == 'Gold') & ((testdf['Position'] == 'Left') | (testdf['Position'] == 'Right') | (testdf['Position'] == 'Down'))  ].index\n",
    "        testdf.drop(indexes, inplace = True)\n",
    "\n",
    "        print(testdf)\n",
    "        X, y = testdf[['Object', 'Position']], list(testdf['Action Estimates'])\n",
    "        \n",
    "        # Learn the general concepts.\n",
    "        X, y = testdf[['Object', 'Position']], list(testdf['Action Estimates'])\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        testdf['Object'] = le.fit_transform(testdf['Object'])\n",
    "        X = pd.get_dummies(X['Position'])\n",
    "        X['Object'] = testdf['Object']\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        print('Left:\\n')\n",
    "        yhat = model.predict([[0, 1, 0, 0, 0]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 1, 0, 0, 1]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 1, 0, 0, 2]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 1, 0, 0, 3]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 1, 0, 0, 4]])\n",
    "        print(yhat)\n",
    "\n",
    "        print('\\nRight:\\n')\n",
    "        yhat = model.predict([[0, 0, 1, 0, 0]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 1, 0, 1]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 1, 0, 2]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 1, 0, 3]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 1, 0, 4]])\n",
    "        print(yhat)\n",
    "\n",
    "        print('\\nUp:\\n')\n",
    "        yhat = model.predict([[0, 0, 0, 1, 0]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 0, 1, 1]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 0, 1, 2]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 0, 1, 3]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[0, 0, 0, 1, 4]])\n",
    "        print(yhat)\n",
    "\n",
    "        print('\\nDown:\\n')\n",
    "        yhat = model.predict([[1, 0, 0, 0, 0]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[1, 0, 0, 0, 1]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[1, 0, 0, 0, 2]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[1, 0, 0, 0, 3]])\n",
    "        print(yhat)\n",
    "        yhat = model.predict([[1, 0, 0, 0, 4]])\n",
    "        print(yhat)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def policy(state, visited_states_, actions_taken_, previous_action, previous_state, next_state):\n",
    "        list_of_positions = ['At', 'Left', 'Top Left', 'Up', 'Top Right', 'Right', 'Bottom Right', 'Down',\n",
    "                             'Bottom Left']\n",
    "        action_mapping = {'At': None, 'Left': 1, 'Top Left': 2, 'Up': 2, 'Top Right': 0, 'Right': 0, 'Bottom Right': 0,\n",
    "                          'Down': 3, 'Bottom Left': 1}\n",
    "        \n",
    "        gold_found, wumpus_found = None, []\n",
    "        pit_found, breeze_found, stench_found = [], [], []\n",
    "        index = 0\n",
    "\n",
    "        # Gold\n",
    "        for i in range(9):\n",
    "            if state[index + 4] == 1:\n",
    "                gold_found = list_of_positions[i]\n",
    "\n",
    "            if state[index + 3] == 1:\n",
    "                wumpus_found.append(list_of_positions[i])\n",
    "\n",
    "            if state[index + 2] == 1:\n",
    "                pit_found.append(list_of_positions[i])\n",
    "\n",
    "            if state[index + 1] == 1:\n",
    "                stench_found.append(list_of_positions[i])\n",
    "\n",
    "            if state[index + 0] == 1:\n",
    "                breeze_found.append(list_of_positions[i])\n",
    "\n",
    "            index += 5\n",
    "            \n",
    "        actions_to_not_take_1 = [0, 0, 0, 0]\n",
    "        import copy\n",
    "\n",
    "        actions_to_not_take = copy.deepcopy(actions_taken_)\n",
    "        if np.array_equal(previous_state, next_state):\n",
    "            actions_to_not_take[previous_action] = 1\n",
    "\n",
    "        if not np.array_equal(previous_state, next_state):\n",
    "            if previous_action == 0:\n",
    "                actions_to_not_take[1] = 1\n",
    "            if previous_action == 1:\n",
    "                actions_to_not_take[0] = 1\n",
    "            if previous_action == 2:\n",
    "                actions_to_not_take[3] = 1\n",
    "            if previous_action == 3:\n",
    "                actions_to_not_take[2] = 1\n",
    "\n",
    "        if gold_found:\n",
    "            action = action_mapping[gold_found]\n",
    "            \n",
    "        else:\n",
    "            if 'Left' in pit_found or 'Left' in wumpus_found:\n",
    "                actions_to_not_take[1] = 1\n",
    "            if 'Right' in pit_found or 'Right' in wumpus_found:\n",
    "                actions_to_not_take[0] = 1\n",
    "            if 'Up' in pit_found or 'Up' in wumpus_found:\n",
    "                actions_to_not_take[2] = 1\n",
    "            if 'Down' in pit_found or 'Down' in wumpus_found:\n",
    "                actions_to_not_take[3] = 1\n",
    "                \n",
    "            possible_actions = []\n",
    "            for i in range(len(actions_to_not_take)):\n",
    "                if actions_to_not_take[i] == 0:\n",
    "                    possible_actions.append(i)\n",
    "\n",
    "            if len(possible_actions) > 0:\n",
    "                action = np.random.choice(possible_actions)\n",
    " \n",
    "            else:\n",
    "                if 'Left' in pit_found or 'Left' in wumpus_found:\n",
    "                    actions_to_not_take_1[1] = 1\n",
    "                if 'Right' in pit_found or 'Right' in wumpus_found:\n",
    "                    actions_to_not_take_1[0] = 1\n",
    "                if 'Up' in pit_found or 'Up' in wumpus_found:\n",
    "                    actions_to_not_take_1[2] = 1\n",
    "                if 'Down' in pit_found or 'Down' in wumpus_found:\n",
    "                    actions_to_not_take_1[3] = 1\n",
    "                    \n",
    "                possible_actions_1 = []\n",
    "                for i in range(len(actions_to_not_take_1)):\n",
    "                    if actions_to_not_take_1[i] == 0:\n",
    "                        possible_actions_1.append(i)\n",
    "\n",
    "                action = np.random.choice(possible_actions_1)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"This method evaluates the performance of the agent after it has finished training.\"\"\"\n",
    "\n",
    "        total_steps, total_penalties = 0, 0  # Initializing the total steps taken and total penalties incurred\n",
    "                                             # across all episodes.\n",
    "        episodes = 100  # Number of episodes for which we are going to test the agent's performance.\n",
    "        rewards_per_episode = []  # Sum of immediate rewards during the episode.\n",
    "        gold = 0  # Counter to keep track of the episodes in which the agent reaches the Gold.\n",
    "        cumulative_rewards_evaluation = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            # Resetting the environment for every new episode.\n",
    "            visited_states = {}\n",
    "            state = self.test_environment.reset(random_start=True)\n",
    "            previous_state_ = state\n",
    "            next_state_ = None\n",
    "#             self.test_environment.render(plot=True)\n",
    "            visited_states[f'{state}'] = [0 for _ in range(self.test_environment.action_space.n)]\n",
    "            actions_taken = visited_states[f'{state}']\n",
    "            previous_action = None\n",
    "            steps, penalties = 0, 0  # Initializing the steps taken, and penalties incurred in this episode.\n",
    "            done = False  # Initializing the done parameter indicating the episode termination to be False.\n",
    "            total_reward_episode = 0  # Initializing the total reward acquired in this episode to be 0.\n",
    "        \n",
    "            while not done:\n",
    "                action = self.policy(state, visited_states, visited_states[f'{state}'], previous_action, previous_state_, next_state_)\n",
    "                previous_action = action\n",
    "                actions_taken = visited_states[f'{state}']\n",
    "                for i in range(self.test_environment.action_space.n):\n",
    "                    if i == action:\n",
    "                        actions_taken[i] = 1\n",
    "                visited_states[f'{state}'] = actions_taken\n",
    "\n",
    "                # Taking the greedy action.\n",
    "                next_state, reward, done, info = self.test_environment.step(action)\n",
    "#                 self.test_environment.render(plot=True)\n",
    "                total_reward_episode += reward  # Adding the reward acquired on this step to the total reward acquired\n",
    "                                                # during the episode.\n",
    "\n",
    "                # Incrementing the Gold counter when the agent reaches the Gold.\n",
    "                if self.test_environment.agent_pos[0] == self.test_environment.gold_pos[0] and \\\n",
    "                        self.test_environment.agent_pos[1] == self.test_environment.gold_pos[1]:\n",
    "                    gold += 1\n",
    "\n",
    "                # Increasing the penalties incurred in this episode by checking the reward.\n",
    "                if reward == -50 or reward == -100:\n",
    "                    penalties += 1\n",
    "                \n",
    "                previous_state_ = state\n",
    "                next_state_ = next_state\n",
    "                state = next_state  # Setting the current state to the next state.\n",
    "                if f'{state}' not in visited_states.keys():\n",
    "                    visited_states[f'{state}'] = [0, 0, 0, 0]\n",
    "    \n",
    "                steps += 1  # Increasing the number of steps taken in this episode.\n",
    "\n",
    "            rewards_per_episode.append(total_reward_episode)  # Appending the reward acquired during the episode.\n",
    "\n",
    "            # Appending the cumulative reward.\n",
    "            if len(cumulative_rewards_evaluation) == 0:\n",
    "                cumulative_rewards_evaluation.append(total_reward_episode)\n",
    "            else:\n",
    "                cumulative_rewards_evaluation.append(\n",
    "                    total_reward_episode + cumulative_rewards_evaluation[episode - 1])\n",
    "\n",
    "            total_penalties += penalties  # Adding the penalties incurred in this episode to the total penalties\n",
    "                                          # across all the episodes.\n",
    "\n",
    "            total_steps += steps  # Adding the steps taken in this episode to the total steps taken across all episodes\n",
    "\n",
    "        # Printing some statistics after the evaluation of agent's performance is completed.\n",
    "        print(f\"\\nEvaluation of agent's performance across {episodes} episodes:\")\n",
    "        print(f\"Average number of steps taken per episode: {total_steps / episodes}\")\n",
    "        print(f\"Average penalties incurred per episode: {total_penalties / episodes}\")\n",
    "        print(f\"Percentage of episodes in which the agent reaches the Gold: {(gold / episodes) * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Jf2SKKeBsnlc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Objective: Gold Primary Objective Reward: 999.9999999999649\n",
      "\n",
      "Secondary Objective: [] Secondary Objective Reward: []\n",
      "\n",
      "Objects to Avoid: ['Pit', 'Wumpus'] Objects to Avoid Rewards: [-49.9999999999997, -99.99999999999929]\n",
      "\n",
      "Situations Unencountered: ['State At Pit', 'State At Wumpus', 'State At Gold', 'State Left Gold', 'State Top Right Wumpus', 'State Right Gold', 'State Bottom Right Wumpus', 'State Bottom Right Gold', 'State Down Gold', 'State Bottom Left Gold', 'Next State Bottom Right Gold', 'Next State Down Gold', 'Next State Bottom Left Gold']\n",
      "\n",
      "\n",
      "    Object Position                                   Action Estimates\n",
      "0   Breeze     Left  [0.9933726435135993, 0.0, 0.001206801242850667...\n",
      "1   Breeze    Right  [0.0, 1.0003167200765528, 0.0, -2.129871412573...\n",
      "2   Breeze       Up                 [0.0, 0.0, 0.999046587687463, 0.0]\n",
      "3   Breeze     Down  [5.247949754243608e-05, 0.00034863611404790105...\n",
      "6     Gold       Up  [1.3691023713525845e-05, 1.3738830057171007e-0...\n",
      "8      Pit     Left  [0.9989764615119442, 0.0, 0.000334846794871494...\n",
      "9      Pit    Right  [1.5437338699750763e-05, 0.9990456152633213, 6...\n",
      "10     Pit       Up  [0.0004106532682009589, 0.0, 0.999075534198474...\n",
      "11     Pit     Down  [0.0, 0.0002370611177711193, 0.0, 0.9991649753...\n",
      "12  Stench     Left  [0.9990011951942191, 0.0, 3.345561642281329e-0...\n",
      "13  Stench    Right  [1.6108998006862775e-05, 0.9990242088039624, 0...\n",
      "14  Stench       Up                [0.0, 0.0, 0.9982489464238706, 0.0]\n",
      "15  Stench     Down  [0.0005014168101523655, 0.00018511449009403358...\n",
      "16  Wumpus     Left  [0.9990097856888893, 0.0, 6.980279502569156e-0...\n",
      "17  Wumpus    Right  [8.870026133365344e-06, 0.9988813143826571, 6....\n",
      "18  Wumpus       Up  [2.576736250169549e-05, 0.0, 0.999009386089761...\n",
      "19  Wumpus     Down  [5.9434302759962954e-05, 5.755753691320511e-06...\n",
      "Left:\n",
      "\n",
      "[[9.96776357e-01 2.40336878e-04 6.05668194e-04 2.69787254e-04]]\n",
      "[[9.97137986e-01 1.33520488e-04 5.12269434e-04 2.92020472e-04]]\n",
      "[[9.97499614e-01 2.67040976e-05 4.18870674e-04 3.14253691e-04]]\n",
      "[[ 9.97861243e-01 -8.01122927e-05  3.25471913e-04  3.36486909e-04]]\n",
      "[[ 9.98222872e-01 -1.86928683e-04  2.32073153e-04  3.58720128e-04]]\n",
      "\n",
      "Right:\n",
      "\n",
      "[[-8.03560492e-04  9.99557302e-01  2.42508262e-04 -2.32088339e-05]]\n",
      "[[-4.41931788e-04  9.99450485e-01  1.49109501e-04 -9.75615352e-07]]\n",
      "[[-8.03030851e-05  9.99343669e-01  5.57107409e-05  2.12576032e-05]]\n",
      "[[ 2.81325618e-04  9.99236852e-01 -3.76880196e-05  4.34908218e-05]]\n",
      "[[ 6.42954322e-04  9.99130036e-01 -1.31086780e-04  6.57240404e-05]]\n",
      "\n",
      "Up:\n",
      "\n",
      "[[-6.33235076e-04  2.16380546e-04  9.99062888e-01 -1.23676061e-05]]\n",
      "[[-2.71606372e-04  1.09564156e-04  9.98969490e-01  9.86561247e-06]]\n",
      "[[9.00223309e-05 2.74776601e-06 9.98876091e-01 3.20988311e-05]]\n",
      "[[ 4.51651034e-04 -1.04068624e-04  9.98782692e-01  5.43320496e-05]]\n",
      "[[ 8.13279738e-04 -2.10885014e-04  9.98689293e-01  7.65652682e-05]]\n",
      "\n",
      "Down:\n",
      "\n",
      "[[-6.60331930e-04  4.34478747e-04  2.37338876e-04  9.98692160e-01]]\n",
      "[[-2.98703227e-04  3.27662357e-04  1.43940115e-04  9.98714393e-01]]\n",
      "[[6.29254768e-05 2.20845966e-04 5.05413549e-05 9.98736626e-01]]\n",
      "[[ 4.24554180e-04  1.14029576e-04 -4.28574055e-05  9.98758860e-01]]\n",
      "[[ 7.86182883e-04  7.21318604e-06 -1.36256166e-04  9.98781093e-01]]\n",
      "\n",
      "Evaluation of agent's performance across 100 episodes:\n",
      "Average number of steps taken per episode: 91.75\n",
      "Average penalties incurred per episode: 0.0\n",
      "Percentage of episodes in which the agent reaches the Gold: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "policy = Policy(wumpus_world_environment_testing)\n",
    "primary_objective, primary_objective_reward = policy.identify_primary_objective()\n",
    "print('Primary Objective:', primary_objective, 'Primary Objective Reward:', primary_objective_reward)\n",
    "secondary_objective, secondary_objective_rewards = policy.identify_secondary_objectives()\n",
    "print('\\nSecondary Objective:', secondary_objective, 'Secondary Objective Reward:', secondary_objective_rewards)\n",
    "objects_to_avoid, objects_to_avoid_rewards = policy.identify_objects_to_avoid()\n",
    "print('\\nObjects to Avoid:', objects_to_avoid, 'Objects to Avoid Rewards:', objects_to_avoid_rewards)\n",
    "situations_unencountered = policy.identify_situations_previously_unencountered()\n",
    "print('\\nSituations Unencountered:', situations_unencountered)\n",
    "print('\\n')\n",
    "policy.generalize_for_unencountered_situations()\n",
    "policy.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "causal_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
